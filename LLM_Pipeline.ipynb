{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install fasttext\n",
        "!pip install huggingface_hub\n",
        "!pip install transformers\n",
        "!pip install torch\n",
        "!pip install pdf2image\n",
        "!pip install pytesseract\n",
        "!pip install python-docx\n",
        "!apt-get install poppler-utils\n",
        "!pip install fpdf\n",
        "!pip install docx\n",
        "!apt-get install tesseract-ocr"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q18y5foTsy8o",
        "outputId": "dd98808f-ffb3-498d-d7a2-6dd2de381356"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: fasttext in /usr/local/lib/python3.10/dist-packages (0.9.3)\n",
            "Requirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.10/dist-packages (from fasttext) (2.13.5)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from fasttext) (71.0.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fasttext) (1.26.4)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.23.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.15.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2024.6.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2024.7.4)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.42.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.5)\n",
            "Requirement already satisfied: numpy<2.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.4)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.7.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (2.3.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.6.20)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: pdf2image in /usr/local/lib/python3.10/dist-packages (1.17.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from pdf2image) (9.4.0)\n",
            "Requirement already satisfied: pytesseract in /usr/local/lib/python3.10/dist-packages (0.3.13)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from pytesseract) (24.1)\n",
            "Requirement already satisfied: Pillow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from pytesseract) (9.4.0)\n",
            "Requirement already satisfied: python-docx in /usr/local/lib/python3.10/dist-packages (1.1.2)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from python-docx) (4.9.4)\n",
            "Requirement already satisfied: typing-extensions>=4.9.0 in /usr/local/lib/python3.10/dist-packages (from python-docx) (4.12.2)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "poppler-utils is already the newest version (22.02.0-2ubuntu0.5).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 45 not upgraded.\n",
            "Requirement already satisfied: fpdf in /usr/local/lib/python3.10/dist-packages (1.7.2)\n",
            "Requirement already satisfied: docx in /usr/local/lib/python3.10/dist-packages (0.2.4)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from docx) (4.9.4)\n",
            "Requirement already satisfied: Pillow>=2.0 in /usr/local/lib/python3.10/dist-packages (from docx) (9.4.0)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  tesseract-ocr-eng tesseract-ocr-osd\n",
            "The following NEW packages will be installed:\n",
            "  tesseract-ocr tesseract-ocr-eng tesseract-ocr-osd\n",
            "0 upgraded, 3 newly installed, 0 to remove and 45 not upgraded.\n",
            "Need to get 4,816 kB of archives.\n",
            "After this operation, 15.6 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-eng all 1:4.00~git30-7274cfa-1.1 [1,591 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-osd all 1:4.00~git30-7274cfa-1.1 [2,990 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr amd64 4.1.1-2.1build1 [236 kB]\n",
            "Fetched 4,816 kB in 1s (5,183 kB/s)\n",
            "Selecting previously unselected package tesseract-ocr-eng.\n",
            "(Reading database ... 123625 files and directories currently installed.)\n",
            "Preparing to unpack .../tesseract-ocr-eng_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-eng (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-osd.\n",
            "Preparing to unpack .../tesseract-ocr-osd_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-osd (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr.\n",
            "Preparing to unpack .../tesseract-ocr_4.1.1-2.1build1_amd64.deb ...\n",
            "Unpacking tesseract-ocr (4.1.1-2.1build1) ...\n",
            "Setting up tesseract-ocr-eng (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-osd (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr (4.1.1-2.1build1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#1 Parse Documents\n",
        "\n",
        "#2 Train Source Material Model\n",
        "\n",
        "#3: Define the Query and Tokenize\n",
        "\n",
        "#4: Retrieve Information Using the Source Material Model\n",
        "\n",
        "#5: Combine and Contextualize Information\n",
        "\n",
        "#6: Compose final answer using Interrogator Model"
      ],
      "metadata": {
        "id": "QibcoOIOF-o0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#1: Parse Documents\n",
        "\n",
        "import pytesseract\n",
        "from pdf2image import convert_from_path\n",
        "import docx\n",
        "import os\n",
        "from fpdf import FPDF\n",
        "from docx import Document\n",
        "\n",
        "# Create a Dummy Directory with Sample Documents\n",
        "\n",
        "# Create a directory for your dummy files\n",
        "dummy_dir = \"dummy_documents\"\n",
        "os.makedirs(dummy_dir, exist_ok=True)\n",
        "\n",
        "def create_updated_word_doc(filepath):\n",
        "    doc = Document()\n",
        "    doc.add_heading('Contract Agreement', 0)\n",
        "    doc.add_paragraph(\"This contract specifies the payment terms as net 45 days.\\nI like to bbq in the summer\")\n",
        "    doc.save(filepath)\n",
        "\n",
        "def create_updated_pdf(filepath):\n",
        "    pdf = FPDF()\n",
        "    pdf.add_page()\n",
        "    pdf.set_font(\"Arial\", size=12)\n",
        "    pdf.cell(200, 10, txt=\"Contract Agreement\", ln=True, align=\"C\")\n",
        "    pdf.multi_cell(0, 10, txt=\"This contract specifies the payment terms as net 45 days.\\nThe project must be completed within 4 months.\")\n",
        "    pdf.output(filepath)\n",
        "\n",
        "# Generate the dummy files with updated content\n",
        "create_updated_word_doc(os.path.join(dummy_dir, \"contract_1.docx\"))\n",
        "create_updated_pdf(os.path.join(dummy_dir, \"contract_2.pdf\"))\n",
        "\n",
        "print(f\"Dummy directory '{dummy_dir}' with updated sample documents created.\")\n",
        "\n",
        "# Parse Documents from the Dummy Directory\n",
        "\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    try:\n",
        "        images = convert_from_path(pdf_path)\n",
        "        text = \"\"\n",
        "        for image in images:\n",
        "            text += pytesseract.image_to_string(image)\n",
        "        return text\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting text from PDF {pdf_path}: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "def extract_text_from_word(docx_path):\n",
        "    try:\n",
        "        doc = docx.Document(docx_path)\n",
        "        text = \"\\n\".join([para.text for para in doc.paragraphs])\n",
        "        return text\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting text from Word document {docx_path}: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "def parse_documents(directory):\n",
        "    document_texts = []\n",
        "    for filename in os.listdir(directory):\n",
        "        filepath = os.path.join(directory, filename)\n",
        "        if filename.endswith(\".pdf\"):\n",
        "            text = extract_text_from_pdf(filepath)\n",
        "        elif filename.endswith(\".docx\"):\n",
        "            text = extract_text_from_word(filepath)\n",
        "        else:\n",
        "            continue  # Skip non-supported files\n",
        "        if text.strip():  # Ensure non-empty text\n",
        "            document_texts.append(text)\n",
        "    return document_texts\n",
        "\n",
        "# Parse the documents in the dummy directory\n",
        "documents = parse_documents(dummy_dir)\n",
        "print(\"Parsed Documents:\")\n",
        "for doc in documents:\n",
        "    print(doc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6WFKgNJXsHL5",
        "outputId": "7d309142-b92c-4567-88ba-46f8d7b5f413"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dummy directory 'dummy_documents' with updated sample documents created.\n",
            "Parsed Documents:\n",
            "Contract Agreement\n",
            "This contract specifies the payment terms as net 45 days.\n",
            "I like to bbq in the summer\n",
            "Contract Agreement\n",
            "This contract specifies the payment terms as net 45 days.\n",
            "\n",
            "The project must be completed within 4 months.\n",
            "\f\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#2: Train Source Material Model\n",
        "\n",
        "from transformers import AutoModelForSequenceClassification, Trainer, TrainingArguments, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "# Prepare the Dataset\n",
        "train_texts = documents  # Parsed documents\n",
        "train_labels = [0] * len(documents)  # Dummy labels (for demonstration purposes)\n",
        "\n",
        "# Load a pre-trained model and tokenizer\n",
        "model_name = \"bert-base-uncased\"  # You can choose another model if needed\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
        "\n",
        "# Tokenize the dataset\n",
        "train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=512)\n",
        "\n",
        "# Create a dataset object\n",
        "class SimpleDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item[\"labels\"] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "train_dataset = SimpleDataset(train_encodings, train_labels)\n",
        "\n",
        "# Set Up Training Arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=4,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=10,  # Updated for better logging granularity\n",
        ")\n",
        "\n",
        "# Initialize and Train the Model\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        ")\n",
        "\n",
        "# Fine-tune the model\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 167
        },
        "id": "zeEMGpj-Jz4U",
        "outputId": "127f9756-0dfd-4087-ec6e-8b2e2836b9c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [3/3 00:50, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=3, training_loss=0.555619478225708, metrics={'train_runtime': 52.0557, 'train_samples_per_second': 0.115, 'train_steps_per_second': 0.058, 'total_flos': 73999984320.0, 'train_loss': 0.555619478225708, 'epoch': 3.0})"
            ]
          },
          "metadata": {},
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#3: Define the Query and Tokenize\n",
        "query = \"What are the payment terms mentioned in the contract?\"\n",
        "\n",
        "# Tokenize the query\n",
        "query_encoding = tokenizer(query, return_tensors='pt')"
      ],
      "metadata": {
        "id": "WDfJB3tKqMF1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#4: Retrieve Information using the Source Material Model\n",
        "def retrieve_information(query_encoding, documents, model, threshold=0.1):  # Lower threshold\n",
        "    retrieved_info = []\n",
        "    for doc in documents:\n",
        "        # Tokenize the document\n",
        "        doc_encoding = tokenizer(doc, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
        "        # Use the model to predict relevance\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**doc_encoding)\n",
        "            logits = outputs.logits\n",
        "            relevance_score = torch.softmax(logits, dim=-1)[0, 1].item()  # Get relevance score\n",
        "\n",
        "            # Debugging: Print out the relevance score for each document\n",
        "            print(f\"Document: {doc[:30]}... Relevance Score: {relevance_score}\")\n",
        "\n",
        "            if relevance_score > threshold:  # Adjusted threshold for relevance\n",
        "                retrieved_info.append((doc, relevance_score))\n",
        "\n",
        "    # Sort by relevance score\n",
        "    retrieved_info.sort(key=lambda x: x[1], reverse=True)\n",
        "    return retrieved_info\n",
        "\n",
        "# Step 3.3: Get the Retrieved Information\n",
        "retrieved_info = retrieve_information(query_encoding, documents, model)\n",
        "\n",
        "# Print the Retrieved Information\n",
        "print(\"Retrieved Information:\")\n",
        "if not retrieved_info:\n",
        "    print(\"No relevant information found.\")\n",
        "else:\n",
        "    for info in retrieved_info:\n",
        "        print(info[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yxnaJj2brz44",
        "outputId": "89e29285-9de9-461f-ac68-a9766ecc7663"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Document: Contract Agreement\n",
            "This contra... Relevance Score: 0.4120257496833801\n",
            "Document: Contract Agreement\n",
            "This contra... Relevance Score: 0.31756943464279175\n",
            "Retrieved Information:\n",
            "Contract Agreement\n",
            "This contract specifies the payment terms as net 45 days.\n",
            "I like to bbq in the summer\n",
            "Contract Agreement\n",
            "This contract specifies the payment terms as net 45 days.\n",
            "\n",
            "The project must be completed within 4 months.\n",
            "\f\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#5: Combine and Contextualize Information\n",
        "\n",
        "def combine_and_contextualize_info(retrieved_info, summary_model, summary_tokenizer, max_length=200):\n",
        "    combined_text = \"\\n\".join([info[0] for info in retrieved_info])\n",
        "    print(\"Combined Text:\")\n",
        "    print(combined_text)\n",
        "\n",
        "    # Prepare the input for the summarization model\n",
        "    inputs = summary_tokenizer.encode(\"summarize: \" + combined_text, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
        "\n",
        "    # Generate the summary\n",
        "    summary_ids = summary_model.generate(\n",
        "        inputs,\n",
        "        max_length=max_length,\n",
        "        min_length=10,  # Ensure minimum length to avoid overly short summaries\n",
        "        length_penalty=2.0,\n",
        "        num_beams=4,\n",
        "        early_stopping=True,\n",
        "        do_sample=True,         # Enable sampling\n",
        "        temperature=0.1,        # Adjust temperature for balance between randomness and coherence\n",
        "        top_p=0.9               # Nucleus sampling to include top-p probability mass\n",
        "    )\n",
        "\n",
        "    summary = summary_tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "    return summary\n",
        "\n",
        "# Get the Contextualized Answer\n",
        "contextualized_answer = combine_and_contextualize_info(retrieved_info, summary_model, summary_tokenizer)\n",
        "print(\"Contextualized Answer:\")\n",
        "print(contextualized_answer)\n"
      ],
      "metadata": {
        "id": "0kT-uM1OqX49",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1cdd4a3f-4b49-4866-d051-3afd255dfadd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Combined Text:\n",
            "Contract Agreement\n",
            "This contract specifies the payment terms as net 45 days.\n",
            "I like to bbq in the summer\n",
            "Contract Agreement\n",
            "This contract specifies the payment terms as net 45 days.\n",
            "\n",
            "The project must be completed within 4 months.\n",
            "\f\n",
            "Contextualized Answer:\n",
            "The project must be completed within 4 months. The payment terms are net 45 days.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#6: Compose final answer using Interrogator Model\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
        "import torch\n",
        "\n",
        "# Load pre-trained models and tokenizers\n",
        "qa_model_name = \"distilbert-base-uncased-distilled-squad\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(qa_model_name)\n",
        "qa_model = AutoModelForQuestionAnswering.from_pretrained(qa_model_name)\n",
        "\n",
        "def get_answer(context, question):\n",
        "    # Tokenize the input\n",
        "    inputs = tokenizer.encode_plus(question, context, add_special_tokens=True, return_tensors=\"pt\")\n",
        "\n",
        "    # Get the model's prediction\n",
        "    with torch.no_grad():\n",
        "        outputs = qa_model(**inputs)\n",
        "        start_scores = outputs.start_logits\n",
        "        end_scores = outputs.end_logits\n",
        "\n",
        "    # Get the most likely start and end token positions\n",
        "    start_index = torch.argmax(start_scores)\n",
        "    end_index = torch.argmax(end_scores) + 1\n",
        "\n",
        "    # Decode the answer from the token indices\n",
        "    answer_tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0][start_index:end_index])\n",
        "    answer = tokenizer.convert_tokens_to_string(answer_tokens)\n",
        "\n",
        "    return answer\n",
        "\n",
        "# Get the final answer\n",
        "final_answer = get_answer(contextualized_answer, query)\n",
        "\n",
        "# Present the final answer\n",
        "def present_answer(answer, query):\n",
        "    \"\"\"\n",
        "    Function to present the final answer to the user in a more user-friendly way.\n",
        "    \"\"\"\n",
        "    # Format the answer\n",
        "    formatted_answer = f\"Question: {query}\\n\\nAnswer:\\n{answer}\"\n",
        "\n",
        "    # Print the formatted answer to the console\n",
        "    print(formatted_answer)\n",
        "\n",
        "# Present the answer\n",
        "present_answer(final_answer, query)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZfryBkSiaj4C",
        "outputId": "6a33db63-505c-4c46-de9c-2ca37a0332a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: What are the payment terms mentioned in the contract?\n",
            "\n",
            "Answer:\n",
            "net 45 days\n"
          ]
        }
      ]
    }
  ]
}